{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    " - Paper [Link](https://arxiv.org/pdf/1901.08634.pdf)<br>\n",
    " - GitHub [Link](https://github.com/google-research/language/tree/master/language/question_answering/bert_joint)<br>\n",
    " - Discussion [Link1](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/119957), [Link2](https://www.kaggle.com/c/tensorflow2-question-answering/discussion/117370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions\n",
    " - xl001<br>\n",
    " Baseline Built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flags\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS.VERSION = 'XLNet001'\n",
    "FLAGS.DATA_SPLIT = False\n",
    "FLAGS.TUNING_MODE = False\n",
    "FLAGS.PREPROCESS = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, os, gc, sys, collections, itertools, datetime, logging, warnings, tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.extend([FLAGS.SACREMOSES_PATH, FLAGS.TRANSFORMERS_PATH])\n",
    "import sacremoses, transformers, tokenization\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.CRITICAL)\n",
    "logging.getLogger(\"tensorflow_hub\").setLevel(logging.CRITICAL)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path, sample = True, chunksize = 80_000):\n",
    "    if not sample:\n",
    "        chunksize = 307_373\n",
    "    df = []\n",
    "    with open(path, 'rt') as reader:\n",
    "        for i in range(chunksize):\n",
    "            df.append(json.loads(reader.readline()))\n",
    "    df = pd.DataFrame(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 46 µs, sys: 4 µs, total: 50 µs\n",
      "Wall time: 55.8 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################\n",
    "# train data\n",
    "################\n",
    "if FLAGS.DATA_SPLIT:\n",
    "    \n",
    "    train = read_data(FLAGS.LOCAL_PATH+'/simplified-nq-train.jsonl', sample = FLAGS.TUNING_MODE)\n",
    "    print(\"train shape\", train.shape)\n",
    "    display(train.head(5))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample submission shape (692, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example_id</th>\n",
       "      <th>PredictionString</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1011141123527297803_long</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1011141123527297803_short</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   example_id  PredictionString\n",
       "0   -1011141123527297803_long               NaN\n",
       "1  -1011141123527297803_short               NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.4 ms, sys: 3.84 ms, total: 34.3 ms\n",
      "Wall time: 32 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "################\n",
    "# sample_submission data\n",
    "################\n",
    "sample_submission = pd.read_csv(FLAGS.LOCAL_PATH+'/sample_submission.csv')\n",
    "print(\"Sample submission shape\", sample_submission.shape)\n",
    "display(sample_submission.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Split Dataset to Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.99 ms, sys: 82 µs, total: 5.08 ms\n",
      "Wall time: 4.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "########################\n",
    "# Train-Validation Split\n",
    "########################\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "if FLAGS.TUNING_MODE:\n",
    "    train_file = FLAGS.LOCAL_PATH+f'/simplified-nq-train_Seed{FLAGS.SEED}_Split{FLAGS.N_SPLITS}_Fold{FLAGS.FOLD}-small.jsonl'\n",
    "    valid_file = FLAGS.LOCAL_PATH+f'/simplified-nq-valid_Seed{FLAGS.SEED}_Split{FLAGS.N_SPLITS}_Fold{FLAGS.FOLD}-small.jsonl'\n",
    "else:\n",
    "    train_file = FLAGS.LOCAL_PATH+f'/simplified-nq-train.jsonl'\n",
    "\n",
    "if FLAGS.DATA_SPLIT and FLAGS.TUNING_MODE:\n",
    "    \n",
    "    #### Stratify Key ####\n",
    "    has_long_answer = train.annotations.apply(\n",
    "        lambda x: (x[0]['long_answer']['start_token'], x[0]['long_answer']['end_token'])).apply(\n",
    "        lambda x: x if x != (-1, -1) else None).apply(\n",
    "        lambda x: 1 if x else 0)\n",
    "    has_short_answers = train.annotations.apply(\n",
    "        lambda x: [(y['start_token'], y['end_token']) for y in x[0]['short_answers']]).apply(\n",
    "        lambda x: len(x)).apply(\n",
    "        lambda x: x > 0)\n",
    "    has_yes_no = train.annotations.apply(\n",
    "        lambda x: x[0]['yes_no_answer']).apply(\n",
    "        lambda x: None if x == 'NONE' else x).apply(\n",
    "        lambda x: x is not None)\n",
    "    \n",
    "    stratify_key = has_long_answer.astype(int).astype(str) + \\\n",
    "                    has_short_answers.astype(int).astype(str) + \\\n",
    "                    has_yes_no.astype(int).astype(str)\n",
    "    \n",
    "    #### StratifiedShuffleSplit ####\n",
    "    sss = StratifiedShuffleSplit(n_splits=FLAGS.N_SPLITS, random_state=FLAGS.SEED)\n",
    "    trn_idx, val_idx = [\n",
    "        (trn_idx, val_idx) for trn_idx, val_idx in sss.split(train, stratify_key)\n",
    "    ][FOLD]\n",
    "    \n",
    "    train_df = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_df = train.loc[val_idx].reset_index(drop=True)\n",
    "    del_gc(train)\n",
    "    \n",
    "    # To avoid OverflowError : annotations - annotation_id to string values\n",
    "    train_df['annotations'] = train_df.annotations.apply(\n",
    "        lambda x: {\n",
    "            'yes_no_answer':x[0]['yes_no_answer'], \n",
    "            'long_answer':x[0]['long_answer'], \n",
    "            'short_answers':x[0]['short_answers'], \n",
    "            'annotation_id':str(x[0]['annotation_id'])}\n",
    "    )\n",
    "    valid_df['annotations'] = valid_df.annotations.apply(\n",
    "        lambda x: {\n",
    "            'yes_no_answer':x[0]['yes_no_answer'], \n",
    "            'long_answer':x[0]['long_answer'], \n",
    "            'short_answers':x[0]['short_answers'], \n",
    "            'annotation_id':str(x[0]['annotation_id'])}\n",
    "    )\n",
    "    \n",
    "    #### Write ####\n",
    "    train_df.to_json(\n",
    "        path_or_buf=train_file, \n",
    "        orient='records',\n",
    "        lines=True\n",
    "    )\n",
    "    valid_df.to_json(\n",
    "        path_or_buf=valid_file, \n",
    "        orient='records',\n",
    "        lines=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 XLNet-Joint Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of added tokens :  207\n",
      "Reading: /home/ec2-user/SageMaker/input/simplified-nq-train.jsonl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01b97cdcde74ae4a2432e65bf5b17bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tokenization import FullTokenizer\n",
    "from preprocessing import ConvertExamples2Features, FeatureWriter, nq_examples_iter\n",
    "\n",
    "tokenizer = FullTokenizer(\n",
    "    vocab_file=None, do_lower_case=True,\n",
    "    spm_model_file=FLAGS.TOKENIZER_MODEL_PATH_LARGE)\n",
    "\n",
    "with open(FLAGS.ADDITIONAL_VOCAB_FILE) as f:\n",
    "    additional_tokens = f.read().splitlines()\n",
    "    \n",
    "additional_token_dict = {}\n",
    "for i, token in enumerate(additional_tokens):\n",
    "    additional_token_dict[token] = i+len(tokenizer.vocab)\n",
    "    \n",
    "print('Number of added tokens : ', len(additional_tokens))\n",
    "\n",
    "if FLAGS.TUNING_MODE:\n",
    "    train_records = FLAGS.LOCAL_PATH+f'/nq-train_<{FLAGS.MODEL_VERSION}>_Seed{FLAGS.SEED}_Split{FLAGS.N_SPLITS}_Fold{FLAGS.FOLD}-small.tfrecords'\n",
    "    valid_records = FLAGS.LOCAL_PATH+f'/nq-valid_<{FLAGS.MODEL_VERSION}>_Seed{FLAGS.SEED}_Split{FLAGS.N_SPLITS}_Fold{FLAGS.FOLD}-small.tfrecords'\n",
    "    input_records = [train_records, valid_records]\n",
    "    input_files = [train_file, valid_file]\n",
    "else:\n",
    "    train_records = FLAGS.LOCAL_PATH+f'/nq-train_<{FLAGS.MODEL_VERSION}>.tfrecords'\n",
    "    input_records = [train_records]\n",
    "    input_files = [train_file]\n",
    "\n",
    "if FLAGS.PREPROCESS:\n",
    "    \n",
    "    for i, records in enumerate(input_records):\n",
    "    \n",
    "        writer = FeatureWriter(\n",
    "            filename = os.path.join(records), \n",
    "            is_training = True)\n",
    "\n",
    "        converter = ConvertExamples2Features(\n",
    "            tokenizer=tokenizer,\n",
    "            is_training=True,\n",
    "            output_fn=writer.process_feature,\n",
    "            additional_token_dict=additional_token_dict, \n",
    "            collect_stat=False)\n",
    "        \n",
    "        n_examples = 0\n",
    "\n",
    "        tqdm_notebook = tqdm.tqdm_notebook\n",
    "        for examples in nq_examples_iter(input_file=input_files[i], is_training=True, tqdm=tqdm_notebook):\n",
    "            for example in examples:\n",
    "                n_examples += converter(example)\n",
    "\n",
    "        writer.close()\n",
    "        print('number of test examples: %d, written to file: %d' % (n_examples,eval_writer.num_features))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tf_record_file, seq_length, batch_size=1, shuffle_buffer_size=0, is_training=False):\n",
    "\n",
    "    if is_training:\n",
    "        features = {\n",
    "            \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.float32),\n",
    "            \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"start_positions\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"end_positions\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"answer_types\": tf.io.FixedLenFeature([], tf.int64)\n",
    "        }\n",
    "    else:\n",
    "        features = {\n",
    "            \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n",
    "            \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.float32),\n",
    "            \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64)\n",
    "        }        \n",
    "\n",
    "    # Taken from the TensorFlow models repository: https://github.com/tensorflow/models/blob/befbe0f9fe02d6bc1efb1c462689d069dae23af1/official/nlp/bert/input_pipeline.py#L24\n",
    "    def decode_record(record, features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        example = tf.io.parse_single_example(record, features)\n",
    "\n",
    "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "        # So cast all int64 to int32.\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.cast(t, tf.int32)\n",
    "            example[name] = t\n",
    "        return example\n",
    "\n",
    "    def select_data_from_record(record):\n",
    "        \n",
    "        x = {\n",
    "            'unique_ids': record['unique_ids'],\n",
    "            'input_ids': record['input_ids'],\n",
    "            'input_mask': record['input_mask'],\n",
    "            'segment_ids': record['segment_ids']\n",
    "        }\n",
    "\n",
    "        if is_training:\n",
    "            y = {\n",
    "                'unique_ids': record['unique_ids'],\n",
    "                'start_positions': record['start_positions'],\n",
    "                'end_positions': record['end_positions'],\n",
    "                'answer_types': record['answer_types']\n",
    "            }\n",
    "\n",
    "            return (x, y)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(tf_record_file)\n",
    "    \n",
    "    dataset = dataset.map(lambda record: decode_record(record, features))\n",
    "    dataset = dataset.map(select_data_from_record)\n",
    "    \n",
    "    if shuffle_buffer_size > 0:\n",
    "        dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFXLNetMainLayer, TFXLNetPreTrainedModel, TFSequenceSummary\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "\n",
    "TFXLNetPreTrainedModel.pretrained_model_archive_map = {\n",
    "    'xlnet-base-cased': \"/home/ec2-user/SageMaker/src/XLNet_NaturalQuestion/xlnet-base-cased-tf_model.h5\",\n",
    "    'xlnet-large-cased': \"/home/ec2-user/SageMaker/src/XLNet_NaturalQuestion/xlnet-large-cased-tf_model.h5\",\n",
    "}\n",
    "\n",
    "class TFXLNetForQuestionAnswering(TFXLNetPreTrainedModel):\n",
    "    \n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(TFXLNetForQuestionAnswering, self).__init__(config, *inputs, **kwargs)\n",
    "        self.transformer = TFXLNetMainLayer(config, name=\"transformer\")\n",
    "        self.qa_outputs = tf.keras.layers.Dense(\n",
    "            config.num_labels, kernel_initializer=get_initializer(config.initializer_range), name=\"qa_outputs\"\n",
    "        )\n",
    "        self.sequence_summary = TFSequenceSummary(\n",
    "            config, initializer_range=config.initializer_range, name=\"sequence_summary\"\n",
    "        )\n",
    "        self.logits_proj = tf.keras.layers.Dense(\n",
    "            FLAGS.NUM_LABELS, kernel_initializer=get_initializer(config.initializer_range), name=\"logits_proj\"\n",
    "        )\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        \n",
    "        transformer_outputs = self.transformer(inputs, **kwargs)\n",
    "        sequence_output = transformer_outputs[0]\n",
    "        \n",
    "        ### Sequence Classification ###\n",
    "        answer_type_logits = self.sequence_summary(sequence_output)\n",
    "        answer_type_logits = self.logits_proj(answer_type_logits)\n",
    "\n",
    "        ### Natural Question ###\n",
    "        logits = self.qa_outputs(sequence_output)\n",
    "        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n",
    "        start_logits = tf.squeeze(start_logits, axis=-1)\n",
    "        end_logits = tf.squeeze(end_logits, axis=-1)\n",
    "        \n",
    "        outputs = (start_logits, end_logits,answer_type_logits,) + transformer_outputs[1:]\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAGS.BASE_MODEL:\n",
    "    model = TFXLNetForQuestionAnswering.from_pretrained('xlnet-base-cased')\n",
    "else:\n",
    "    model = TFXLNetForQuestionAnswering.from_pretrained('xlnet-large-cased')\n",
    "\n",
    "print('SUMMARY')\n",
    "display(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Fitting Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def loss_function(nq_labels, nq_logits):\n",
    "    \n",
    "    (start_pos_labels, end_pos_labels, answer_type_labels) = nq_labels\n",
    "    (start_pos_logits, end_pos_logits, answer_type_logits) = nq_logits\n",
    "    start_pos_labels = tf.dtypes.cast(start_pos_labels, tf.float32)\n",
    "    end_pos_labels = tf.dtypes.cast(end_pos_labels, tf.float32)\n",
    "    answer_type_labels = tf.dtypes.cast(answer_type_labels, tf.float32)\n",
    "    \n",
    "    loss_start_pos = loss_object(start_pos_labels, start_pos_logits)\n",
    "    loss_end_pos = loss_object(end_pos_labels, end_pos_logits)\n",
    "    loss_ans_type = loss_object(answer_type_labels, answer_type_logits)\n",
    "    \n",
    "    loss_start_pos = tf.math.reduce_sum(loss_start_pos)\n",
    "    loss_end_pos = tf.math.reduce_sum(loss_end_pos)\n",
    "    loss_ans_type = tf.math.reduce_sum(loss_ans_type)\n",
    "    \n",
    "    loss = (loss_start_pos + loss_end_pos + loss_ans_type) / 3.0\n",
    "    \n",
    "    return loss, loss_start_pos, loss_end_pos, loss_ans_type\n",
    "\n",
    "\n",
    "def get_loss_and_gradients(unique_ids, input_ids, input_masks, segment_ids, start_pos_labels, \n",
    "                           end_pos_labels, answer_type_labels):\n",
    "    \n",
    "#     nq_inputs = (unique_ids, input_ids, input_masks, segment_ids)\n",
    "    nq_labels = (start_pos_labels, end_pos_labels, answer_type_labels)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        nq_logits = model(input_ids, input_mask=input_masks, token_type_ids=segment_ids, training=True)\n",
    "        loss, loss_start_pos, loss_end_pos, loss_ans_type = loss_function(nq_labels, nq_logits)\n",
    "                \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)        \n",
    "    \n",
    "    (start_pos_logits, end_pos_logits, answer_type_logits) = nq_logits\n",
    "        \n",
    "    train_acc.update_state(start_pos_labels, start_pos_logits)\n",
    "    train_acc.update_state(end_pos_labels, end_pos_logits)\n",
    "    train_acc.update_state(answer_type_labels, answer_type_logits)\n",
    "\n",
    "    train_acc_start_pos.update_state(start_pos_labels, start_pos_logits)\n",
    "    train_acc_end_pos.update_state(end_pos_labels, end_pos_logits)\n",
    "    train_acc_ans_type.update_state(answer_type_labels, answer_type_logits)\n",
    "    \n",
    "    return loss, gradients, loss_start_pos, loss_end_pos, loss_ans_type\n",
    "\n",
    "\n",
    "def get_metrics(name):\n",
    "\n",
    "    loss = tf.keras.metrics.Mean(name=f'{name}_loss')\n",
    "    loss_start_pos = tf.keras.metrics.Mean(name=f'{name}_loss_start_pos')\n",
    "    loss_end_pos = tf.keras.metrics.Mean(name=f'{name}_loss_end_pos')\n",
    "    loss_ans_type = tf.keras.metrics.Mean(name=f'{name}_loss_ans_type')\n",
    "    \n",
    "    comp_metric = tf.keras.metrics.CategoricalAccuracy(name=f'{name}_acc')##### ToDo\n",
    "    acc_start_pos = tf.keras.metrics.CategoricalAccuracy(name=f'{name}_acc_start_pos')\n",
    "    acc_end_pos = tf.keras.metrics.CategoricalAccuracy(name=f'{name}_acc_end_pos')\n",
    "    acc_ans_type = tf.keras.metrics.CategoricalAccuracy(name=f'{name}_acc_ans_type')\n",
    "    \n",
    "    return loss, loss_start_pos, loss_end_pos, loss_ans_type, comp_metric, acc_start_pos, acc_end_pos, acc_ans_type\n",
    "\n",
    "\n",
    "def train_step_with_batch_accumulation(unique_ids, input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels):\n",
    "\n",
    "    # This gets None! (probably due to input_signature)\n",
    "    # batch_size = input_ids.shape[0]\n",
    "    \n",
    "    # Try this.\n",
    "    nb_examples = tf.math.reduce_sum(tf.cast(tf.math.not_equal(start_pos_labels, -2), tf.int32))\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_loss_start_pos = 0.0\n",
    "    total_loss_end_pos = 0.0\n",
    "    total_loss_ans_type = 0.0\n",
    "    \n",
    "    total_gradients = [tf.constant(0, shape=x.shape, dtype=tf.float32) for x in model.trainable_variables]        \n",
    "    ### total_gradients_sparse = [tf.IndexedSlices(values=tf.constant(0.0, shape=[1] + x.shape.as_list()[1:]), indices=tf.constant([0], dtype=tf.int32), dense_shape=x.shape.as_list()) for x in model.trainable_variables]        \n",
    "\n",
    "    for idx in tf.range(FLAGS.BATCH_ACCUMULATION_SIZE):    \n",
    "                \n",
    "        start_idx = FLAGS.BATCH_SIZE * idx\n",
    "        end_idx = FLAGS.BATCH_SIZE * (idx + 1)\n",
    "        \n",
    "        if start_idx >= nb_examples:\n",
    "            break\n",
    "\n",
    "        (unique_ids_mini, input_ids_mini, input_masks_mini, segment_ids_mini) = (unique_ids[start_idx:end_idx], input_ids[start_idx:end_idx], input_masks[start_idx:end_idx], segment_ids[start_idx:end_idx])\n",
    "        (start_pos_labels_mini, end_pos_labels_mini, answer_type_labels_mini) = (start_pos_labels[start_idx:end_idx], end_pos_labels[start_idx:end_idx], answer_type_labels[start_idx:end_idx])\n",
    "        \n",
    "        loss, gradients, loss_start_pos, loss_end_pos, loss_ans_type = get_loss_and_gradients(unique_ids_mini, input_ids_mini, input_masks_mini, segment_ids_mini, start_pos_labels_mini, end_pos_labels_mini, answer_type_labels_mini)\n",
    "        \n",
    "        total_loss += loss\n",
    "        total_loss_start_pos += loss_start_pos\n",
    "        total_loss_end_pos += loss_end_pos\n",
    "        total_loss_ans_type += loss_ans_type\n",
    "        \n",
    "        print(len(gradients))\n",
    "        total_gradients = [x + y for x, y in zip(total_gradients, gradients)]        \n",
    "        ### total_gradients_sparse = [_add_grads_for_var(x, y) for x, y in zip(total_gradients_sparse, gradients)]\n",
    "\n",
    "    average_loss = tf.math.divide(total_loss, tf.cast(nb_examples, tf.float32))        \n",
    "    average_gradients = [tf.divide(x, tf.cast(nb_examples, tf.float32)) for x in total_gradients]\n",
    "    ### average_gradients_sparse = [tf.scalar_mul(tf.divide(1.0, tf.cast(nb_examples, tf.float32)), x) for x in total_gradients_sparse]\n",
    "    \n",
    "    optimizer.apply_gradients(zip(average_gradients, model.trainable_variables))\n",
    "    ### optimizer.apply_gradients(zip(average_gradients_sparse, model.trainable_variables))\n",
    "\n",
    "    average_loss_start_pos = tf.math.divide(total_loss_start_pos, tf.cast(nb_examples, tf.float32))\n",
    "    average_loss_end_pos = tf.math.divide(total_loss_end_pos, tf.cast(nb_examples, tf.float32))\n",
    "    average_loss_ans_type = tf.math.divide(total_loss_ans_type, tf.cast(nb_examples, tf.float32))    \n",
    "    \n",
    "    train_loss(average_loss)\n",
    "    train_loss_start_pos(average_loss_start_pos)\n",
    "    train_loss_end_pos(average_loss_end_pos)\n",
    "    train_loss_ans_type(average_loss_ans_type)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizer import AdamW, CustomSchedule\n",
    "\n",
    "train_dataset = get_dataset(\n",
    "    train_records,\n",
    "    seq_length=FLAGS.max_seq_length,\n",
    "    batch_size=FLAGS.BATCH_SIZE*FLAGS.BATCH_ACCUMULATION_SIZE,\n",
    "    shuffle_buffer_size=500_000,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "train_loss, train_loss_start_pos, train_loss_end_pos, train_loss_ans_type, train_acc, train_acc_start_pos, train_acc_end_pos, train_acc_ans_type = get_metrics(\"train\")\n",
    "valid_loss, valid_loss_start_pos, valid_loss_end_pos, valid_loss_ans_type, valid_acc, valid_acc_start_pos, valid_acc_end_pos, valid_acc_ans_type = get_metrics(\"valid\")\n",
    "\n",
    "num_training_examples = 72_000 if FLAGS.TUNING_MODE else 0\n",
    "num_train_steps = int(FLAGS.EPOCHS * num_training_examples / FLAGS.BATCH_SIZE / FLAGS.BATCH_ACCUMULATION_SIZE)\n",
    "learning_rate = CustomSchedule(\n",
    "    initial_learning_rate=FLAGS.LEARNING_RATE,\n",
    "    decay_steps=num_train_steps,\n",
    "    end_learning_rate=0.0,\n",
    "    power=1.0,\n",
    "    cycle=False,    \n",
    "    num_warmup_steps=500\n",
    ")\n",
    "\n",
    "decay_var_list = []\n",
    "for i in range(len(model.trainable_variables)):\n",
    "    name = model.trainable_variables[i].name\n",
    "    if any(x in name for x in [\"LayerNorm\", \"layer_norm\", \"bias\"]):\n",
    "        decay_var_list.append(name)\n",
    "        \n",
    "optimizer = AdamW(weight_decay=0.01, learning_rate=learning_rate, \n",
    "                  beta_1=0.9, beta_2=0.999, epsilon=1e-6, decay_var_list=decay_var_list)\n",
    "\n",
    "checkpoint_path = os.path.join(FLAGS.WEIGHTS_PATH, f'model_{FLAGS.VERSION}.ckpt')\n",
    "ckpt = tf.train.Checkpoint(model=model, optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "train_step = train_step_with_batch_accumulation\n",
    "\n",
    "train_start_time = datetime.datetime.now()\n",
    "\n",
    "for epoch in range(FLAGS.EPOCHS):\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_loss_start_pos.reset_states()\n",
    "    train_loss_end_pos.reset_states()\n",
    "    train_loss_ans_type.reset_states()    \n",
    "    \n",
    "    train_acc.reset_states()\n",
    "    train_acc_start_pos.reset_states()\n",
    "    train_acc_end_pos.reset_states()\n",
    "    train_acc_ans_type.reset_states()\n",
    "    \n",
    "    epoch_start_time = datetime.datetime.now()\n",
    "    \n",
    "    for (batch_idx, (features, targets)) in tqdm.tqdm_notebook(enumerate(train_dataset)):\n",
    "        \n",
    "        (unique_ids, input_ids, input_masks, segment_ids) = (features['unique_ids'], features['input_ids'], features['input_mask'], features['segment_ids'])\n",
    "        (_, start_pos_labels, end_pos_labels, answer_type_labels) = (targets['unique_ids'], targets['start_positions'], targets['end_positions'], targets['answer_types'])\n",
    "    \n",
    "        batch_start_time = datetime.datetime.now()\n",
    "        \n",
    "        train_step(unique_ids, input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels)\n",
    "\n",
    "        batch_end_time = datetime.datetime.now()\n",
    "        batch_elapsed_time = (batch_end_time - batch_start_time).total_seconds()\n",
    "        \n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print('Epoch {} | Batch {} | Elapsed Time {}'.format(\n",
    "                epoch + 1,\n",
    "                batch_idx + 1,\n",
    "                batch_elapsed_time\n",
    "            ))\n",
    "            print('Loss {:.6f} | Loss_S {:.6f} | Loss_E {:.6f} | Loss_T {:.6f}'.format(\n",
    "                train_loss.result(),\n",
    "                train_loss_start_pos.result(),\n",
    "                train_loss_end_pos.result(),\n",
    "                train_loss_ans_type.result()\n",
    "            ))\n",
    "            print(' Acc {:.6f} |  Acc_S {:.6f} |  Acc_E {:.6f} |  Acc_T {:.6f}'.format(\n",
    "                train_acc.result(),\n",
    "                train_acc_start_pos.result(),\n",
    "                train_acc_end_pos.result(),\n",
    "                train_acc_ans_type.result()\n",
    "            ))\n",
    "            print(\"-\" * 100)\n",
    "       \n",
    "    epoch_end_time = datetime.datetime.now()\n",
    "    epoch_elapsed_time = (epoch_end_time - epoch_start_time).total_seconds()\n",
    "            \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        \n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print ('\\nSaving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "        \n",
    "        print('\\nEpoch {}'.format(epoch + 1))\n",
    "        print('Loss {:.6f} | Loss_S {:.6f} | Loss_E {:.6f} | Loss_T {:.6f}'.format(\n",
    "            train_loss.result(),\n",
    "            train_loss_start_pos.result(),\n",
    "            train_loss_end_pos.result(),\n",
    "            train_loss_ans_type.result()\n",
    "        ))\n",
    "        print(' Acc {:.6f} |  Acc_S {:.6f} |  Acc_E {:.6f} |  Acc_T {:.6f}'.format(\n",
    "            train_acc.result(),\n",
    "            train_acc_start_pos.result(),\n",
    "            train_acc_end_pos.result(),\n",
    "            train_acc_ans_type.result()\n",
    "        ))\n",
    "\n",
    "    print('\\nTime taken for 1 epoch: {} secs\\n'.format(epoch_elapsed_time))\n",
    "    print(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    model.save_weights(WEIGHTS_PATH+f'/modelweights_{VERSION}.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
