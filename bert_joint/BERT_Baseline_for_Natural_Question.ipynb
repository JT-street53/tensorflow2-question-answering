{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    " - Trying to Reproduce prvi's [Notebook](https://www.kaggle.com/prokaj/bert-joint-baseline-notebook/)<br>\n",
    " - Paper [Link](https://arxiv.org/pdf/1901.08634.pdf)<br>\n",
    " - GitHub [Link](https://github.com/google-research/language/tree/master/language/question_answering/bert_joint)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions\n",
    " - bJOINT001<br>\n",
    " Baseline Built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"attention_probs_dropout_prob\": 0.1,\n",
      "    \"hidden_act\": \"gelu\",\n",
      "    \"hidden_dropout_prob\": 0.1,\n",
      "    \"hidden_size\": 1024,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 4096,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"type_vocab_size\": 2,\n",
      "    \"vocab_size\": 30522\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import json, os, gc\n",
    "\n",
    "import sys\n",
    "sys.path.extend(['./module/bert-joint-baseline/'])\n",
    "import bert_utils, modeling, tokenization\n",
    "\n",
    "import importlib\n",
    "importlib.reload(bert_utils)\n",
    "\n",
    "with open('./module/bert-joint-baseline/bert_config.json','r') as f:\n",
    "    config = json.load(f)\n",
    "print(json.dumps(config,indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'bJOINT001'\n",
    "LOCAL_PATH = '../input'\n",
    "WEIGHTS_PATH = '../input/weights'\n",
    "\n",
    "BERT_VERSION = 'large'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bert-baseline\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BertModel)                ((None, 1024), (None 335141888   input_ids[0][0]                  \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "logits (TDense)                 (None, 512, 2)       2050        bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_split (TensorFlowOp [(None, 512, 1), (No 0           logits[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "unique_id (InputLayer)          [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_start_squeeze (Tens [(None, 512)]        0           tf_op_layer_split[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_end_squeeze (Tensor [(None, 512)]        0           tf_op_layer_split[0][1]          \n",
      "__________________________________________________________________________________________________\n",
      "ans_type (TDense)               (None, 5)            5125        bert[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 335,149,063\n",
      "Trainable params: 335,149,063\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class TDense(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 output_size,\n",
    "                 kernel_initializer=None,\n",
    "                 bias_initializer=\"zeros\",\n",
    "                **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.output_size = output_size\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.bias_initializer = bias_initializer\n",
    "    def build(self,input_shape):\n",
    "        dtype = tf.as_dtype(self.dtype or tf.keras.backend.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "            raise TypeError(\"Unable to build `TDense` layer with \"\n",
    "                          \"non-floating point (and non-complex) \"\n",
    "                          \"dtype %s\" % (dtype,))\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        if tf.compat.dimension_value(input_shape[-1]) is None:\n",
    "            raise ValueError(\"The last dimension of the inputs to \"\n",
    "                           \"`TDense` should be defined. \"\n",
    "                           \"Found `None`.\")\n",
    "        last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        ### tf 2.1 rc min_ndim=3 -> min_ndim=2\n",
    "        self.input_spec = tf.keras.layers.InputSpec(min_ndim=2, axes={-1: last_dim})\n",
    "        self.kernel = self.add_weight(\n",
    "            \"kernel\",\n",
    "            shape=[self.output_size,last_dim],\n",
    "            initializer=self.kernel_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        self.bias = self.add_weight(\n",
    "            \"bias\",\n",
    "            shape=[self.output_size],\n",
    "            initializer=self.bias_initializer,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True)\n",
    "        super(TDense, self).build(input_shape)\n",
    "    def call(self,x):\n",
    "        return tf.matmul(x,self.kernel,transpose_b=True)+self.bias\n",
    "    \n",
    "def mk_model(config):\n",
    "    seq_len = config['max_position_embeddings']\n",
    "    unique_id  = tf.keras.Input(shape=(1,),dtype=tf.int64,name='unique_id')\n",
    "    input_ids   = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_ids')\n",
    "    input_mask  = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='input_mask')\n",
    "    segment_ids = tf.keras.Input(shape=(seq_len,),dtype=tf.int32,name='segment_ids')\n",
    "    BERT = modeling.BertModel(config=config,name='bert')\n",
    "    pooled_output, sequence_output = BERT(input_word_ids=input_ids,\n",
    "                                          input_mask=input_mask,\n",
    "                                          input_type_ids=segment_ids)\n",
    "    \n",
    "    logits = TDense(2,name='logits')(sequence_output)\n",
    "    start_logits,end_logits = tf.split(logits,axis=-1,num_or_size_splits= 2,name='split')\n",
    "    start_logits = tf.squeeze(start_logits,axis=-1,name='start_squeeze')\n",
    "    end_logits   = tf.squeeze(end_logits,  axis=-1,name='end_squeeze')\n",
    "    \n",
    "    ans_type      = TDense(5,name='ans_type')(pooled_output)\n",
    "    return tf.keras.Model([input_ for input_ in [unique_id,input_ids,input_mask,segment_ids] \n",
    "                           if input_ is not None],\n",
    "                          [unique_id,start_logits,end_logits,ans_type],\n",
    "                          name='bert-baseline')\n",
    "\n",
    "####################\n",
    "# if BERT - small\n",
    "####################\n",
    "if BERT_VERSION=='small':\n",
    "    small_config = config.copy()\n",
    "    small_config['vocab_size']=16\n",
    "    small_config['hidden_size']=64\n",
    "    small_config['max_position_embeddings'] = 32\n",
    "    small_config['num_hidden_layers'] = 4\n",
    "    small_config['num_attention_heads'] = 4\n",
    "    small_config['intermediate_size'] = 256\n",
    "    small_config\n",
    "    model= mk_model(small_config)\n",
    "else:\n",
    "    model= mk_model(config)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Fitting Starts Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
